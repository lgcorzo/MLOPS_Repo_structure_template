{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex ProjectName trasformers for ML using huggingface  notebook\n",
    "\n",
    "<!-- cspell:ignore  Galo Corzo MLOPS jupyter -->\n",
    "<!-- cspell:enable -->\n",
    "\n",
    "| Version | name | Release Date | Description |\n",
    "| ------- |---------| ------------ | ----------- |\n",
    "| 1.0     | Luis Galo Corzo |February 1, 2023 | task-xxxx (yyyy) (Ex) Test trasformers huggingface |\n",
    "<!-- PULL_REQUESTS_TABLE -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " In this notebook, it is the start of the MLOPS process:\n",
    "-  Define te first environment related ti the Business Uderstanding process.\n",
    "-  we will initialize the project by creating a new .env file. This file will contain all the secret parameters and connection details required for the development and testing environments. Please note that access to the production environment is restricted and not allowed from personal computers.\n",
    "-  Read the data needed to star the process, this process wil be realized in the AzureML workspace, the data can not be download to the personal computers, if it is needed will this data has to be anonymized properly "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "<!-- cspell:enable -->\n",
    "<!-- cspell:ignoreRegExp /^(\\s*`{3,}).*[\\s\\S]*?^\\1/gmx -->\n",
    "## Environment installation\n",
    "\n",
    "The environment needed to run the notebook is defined in Settings\\Notebooks\\Experimenting\\Ex_env.yaml\n",
    "\n",
    "The command to install the environment for this notebook is:\n",
    "\n",
    "``` cmd\n",
    "conda env create -f Settings\\Notebooks\\Experimenting\\Ex_env.yaml  \n",
    "```\n",
    "To update the environment the command is:\n",
    "``` cmd\n",
    "conda env update -n BU_env -f Settings\\Notebooks\\Experimenting\\Ex_env.yaml  --prune\n",
    "```\n",
    "\n",
    "the environment can be run in windows or Linux and all the packages used has to be updated in the env file\n",
    "\n",
    "The notebook is a Jupyter notebook so the environment needs the ipykernel package to run from a jupyter server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface remote api for testing trasformes codebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/codebert-base\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query({\n",
    "\t\"inputs\": \"Today is a sunny day and I'll get some ice cream.\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"\"\"O0174\n",
    "(SHEET : L=180 X W=115 X 0.8 Al99)\n",
    "(PR/1)\n",
    "(MC/ACUTE255)\n",
    "(TR/Standard 1)\n",
    "(MA/Al990.80)\n",
    "(PZ/   0.00X    0.00)\n",
    "(WK/    0.80T  180.00X  115.00)\n",
    "(CL/ 0200.00  1000.00                  )\n",
    "(BP/   0.00X   0.00)\n",
    "(TT/00H00M30S)\n",
    "(CR/Y2022M06D22)\n",
    "G92X1210. Y1270.\n",
    "G06A0.8 B0\n",
    "M691\n",
    "M692\n",
    "M13\n",
    "G50\n",
    "%\"\"\"\n",
    "\n",
    "doc2 = \"\"\"O0175\n",
    "(SHEET : L=150 X W=100 X 0.6 Al90)\n",
    "(PR/1)\n",
    "(MC/ACUTE250)\n",
    "(TR/Standard 2)\n",
    "(MA/Al900.60)\n",
    "%\"\"\"\n",
    "\n",
    "doc3 = \"\"\" \n",
    "G90\n",
    "G00 X18.78 Y-150.80 F6000.00\n",
    "M23\n",
    "M10\n",
    "G04 F6000.0\n",
    "G01 X20.29 Y-157.63\n",
    "G03 X28.01 Y-155.75 R5.50\n",
    "G02 X68.46 Y-115.30 R110.50\n",
    "G03 X62.96 Y-105.78 R5.50\n",
    "G03 X18.49 Y-150.25 R121.50\n",
    "G03 X20.72 Y-157.88 R5.50\n",
    "G01 X25.49 Y-156.37\n",
    "M11\n",
    "M22\n",
    "G00 X113.71 Y-71.50 F6000.00\n",
    "M23\n",
    "M10\n",
    "G04 F6000.0\n",
    "G01 X111.17 Y-72.03\n",
    "G03 X110.36 Y-74.50 R1.50\n",
    "G03 X116.89 Y-68.32 R4.50\n",
    "G03 X110.71 Y-74.85 R4.50\n",
    "G03 X113.18 Y-74.04 R1.50\n",
    "G01 X113.71 Y-71.50\n",
    "M11\n",
    "M22\n",
    "G00 X133.71 Y-71.50 F6000.00\n",
    "M23\n",
    "M10\n",
    "G04 F6000.0\n",
    "G01 X131.17 Y-72.03\n",
    "G03 X130.36 Y-74.50 R1.50\n",
    "G03 X136.90 Y-68.32 R4.50\n",
    "G03 X130.71 Y-74.85 R4.50\n",
    "G03 X133.18 Y-74.04 R1.50\n",
    "G01 X133.71 Y-71.50\n",
    "M11\n",
    "M22\n",
    "G00 X133.71 Y-50.50 F6000.00\n",
    "M23\n",
    "M10\n",
    "G04 F6000.0\n",
    "G01 X131.17 Y-51.03\n",
    "G03 X130.36 Y-53.50 R1.50\n",
    "G03 X136.90 Y-47.32 R4.50\n",
    "G03 X130.71 Y-53.85 R4.50\n",
    "G03 X133.18 Y-53.04 R1.50\n",
    "G01 X133.71 Y-50.50\n",
    "M11\n",
    "M22\n",
    "\"\"\"\n",
    "\n",
    "doc4=\"\"\"\n",
    "10 EXTERN MACHINE_ON (INT)\n",
    "N11 EXTERN MACHINE_OFF (INT)\n",
    "N12 EXTERN LASER_ON (INT)\n",
    "N13 EXTERN LASER_OFF (INT)\n",
    "N14 EXTERN LASER_STARTP (INT)\n",
    "N15 EXTERN ORIGEM (INT,INT)\n",
    "N16 EXTERN TRANSF (INT,INT)\n",
    "N17 ; CHAPA : 295X164X0.8 MATERIAL : Al99\n",
    "N18 ; ORDEM : (Smartdeploy -> JOB000003)\n",
    "N19 INIT_VAR\n",
    "N20 SS0800\n",
    "N21 MACHINE_ON(0)\n",
    "N22 ;ORIGEM(295,164)\n",
    "N23 ;TRANSF (PX,PY)\n",
    "N24 G53G00Z95\n",
    "N25 ;GOTOF \"PZN\"<<PZN\n",
    "N26 PZN1:PZN=\"1\";PECA=DXFPUNC0 SUB_1 pz1\n",
    "N27 TRANS X=1 Y=1\n",
    "N28 REPEAT SUB_1 END\n",
    "N29 END_VAR\n",
    "N30 G53G00Z95\n",
    "N31 MACHINE_OFF(0)\n",
    "N32 M30\n",
    "\n",
    "N1000 ;PECA=DXFPUNC0\n",
    "N1001 SUB_1:\n",
    "N1002 SUB1_1:SUB=\"1_1\"; (DXFPUNC0) pz1 Cont=1\n",
    "N1003 G00Z40\n",
    "N1004 G00X50.173Y25.4F130000\n",
    "N1005 LASER_STARTP(1)\n",
    "N1006 LASER_ON(6)\n",
    "N1007 G01X49.278Y26.95\n",
    "N1008 LASER_ON(1)\n",
    "N1009 G02X50Y28.2I0.722J0.417\n",
    "N1010 Y22.4I0J-2.9\n",
    "N1011 Y28.2I0J2.9\n",
    "N1012 LASER_OFF(0)\n",
    "N1013 SUB1_2:SUB=\"1_2\"; (DXFPUNC0) pz1 Cont=2\n",
    "N1014 G00Z40\n",
    "N1015 G01X39.173Y42.7F130000\n",
    "N1016 LASER_STARTP(1)\n",
    "N1017 LASER_ON(6)\n",
    "N1018 G01X38.524Y43.825\n",
    "N1019 LASER_ON(1)\n",
    "N1020 G02X39Y44.65I0.476J0.275\n",
    "N1021 Y40.55I0J-2.05\n",
    "N1022 Y44.65I0J2.05\n",
    "N1023 LASER_OFF(0)\n",
    "N1024 SUB1_3:SUB=\"1_3\"; (DXFPUNC0) pz1 Cont=3\n",
    "N1025 G00Z40\n",
    "N1026 G01X59.745Y49.072F130000\n",
    "N1027 LASER_STARTP(1)\n",
    "N1028 LASER_ON(6)\n",
    "N1029 G01X59.095Y50.197\n",
    "N1030 LASER_ON(1)\n",
    "N1031 G02X59.571Y51.022I0.476J0.275\n",
    "N1032 Y46.922I0J-2.05\n",
    "N1033 Y51.022I0J2.05\n",
    "N1034 LASER_OFF(0)\n",
    "N1035 SUB1_4:SUB=\"1_4\"; (DXFPUNC0) pz1 Cont=4\n",
    "N1036 G00Z40\n",
    "N1037 G01X53.209Y59.136F130000\n",
    "N1038 LASER_STARTP(1)\n",
    "N1039 LASER_ON(6)\n",
    "N1040 G01X52.747Y59.936\n",
    "N1041 LASER_ON(1)\n",
    "N1042 G02X53.036Y60.436I0.289J0.167\n",
    "N1043 Y57.636I0J-1.4\n",
    "N1044 Y60.436I0J1.4\n",
    "N1045 LASER_OFF(0)\n",
    "N1046 SUB1_5:SUB=\"1_5\"; (DXFPUNC0) pz1 Cont=5\n",
    "N1047 G00Z40\n",
    "N1048 G01X46.673Y69.2F130000\n",
    "N1049 LASER_STARTP(1)\n",
    "N1050 LASER_ON(6)\n",
    "N1051 G01X46.024Y70.325\n",
    "N1052 LASER_ON(1)\n",
    "N1053 G02X46.5Y71.15I0.476J0.275\n",
    "N1054 Y67.05I0J-2.05\n",
    "N1055 Y71.15I0J2.05\n",
    "N1056 LASER_OFF(0)\n",
    "N1057 SUB1_6:SUB=\"1_6\"; (DXFPUNC0) pz1 Cont=6\n",
    "N1058 G00Z40\n",
    "N1059 G01X60.485Y67.394F130000\n",
    "N1060 LASER_STARTP(1)\n",
    "N1061 LASER_ON(6)\n",
    "N1062 G01X62.119Y64.878\n",
    "N1063 LASER_ON(1)\n",
    "N1064 G02X59.938Y63.462I-1.09J-0.708\n",
    "N1065 G01X51.088Y77.091\n",
    "N1066 G02X59.139Y82.319I4.026J2.614\n",
    "N1067 G01X76.84Y55.063\n",
    "N1068 G02X68.789Y49.834I-4.026J-2.615\n",
    "N1069 G01X59.938Y63.462\n",
    "N1070 LASER_OFF(0)\n",
    "N1071 SUB1_7:SUB=\"1_7\"; (DXFPUNC0) pz1 Cont=7\n",
    "N1072 G00Z40\n",
    "N1073 G01X39.173Y89.7F130000\n",
    "N1074 LASER_STARTP(1)\n",
    "N1075 LASER_ON(6)\n",
    "N1076 G01X38.524Y90.825\n",
    "N1077 LASER_ON(1)\n",
    "N1078 G02X39Y91.65I0.476J0.275\n",
    "N1079 Y87.55I0J-2.05\n",
    "N1080 Y91.65I0J2.05\n",
    "N1081 LASER_OFF(0)\n",
    "N1082 SUB1_8:SUB=\"1_8\"; (DXFPUNC0) pz1 Cont=8\n",
    "N1083 G00Z40\n",
    "N1084 G01X76.457Y95.31F130000\n",
    "N1085 LASER_STARTP(1)\n",
    "N1086 LASER_ON(6)\n",
    "N1087 G01Y93.143\n",
    "N1088 LASER_ON(1)\n",
    "N1089 G03X77.65Y91.95I1.193J0\n",
    "N1090 G01X84.1\n",
    "N1091 Y98.05\n",
    "N1092 X74.5\n",
    "N1093 Y91.95\n",
    "N1094 X77.65\n",
    "N1095 LASER_OFF(0)\n",
    "N1096 SUB1_9:SUB=\"1_9\"; (DXFPUNC0) pz1 Cont=9\n",
    "N1097 G00Z40\n",
    "N1098 G01X114.887Y65.801F130000\n",
    "N1099 LASER_STARTP(1)\n",
    "N1100 LASER_ON(6)\n",
    "N1101 G01X113.561Y64.476\n",
    "N1102 LASER_ON(1)\n",
    "N1103 G01X118.016Y68.93\n",
    "N1104 G02X118.298Y68.648I0.141J-0.141\n",
    "N1105 G01X113.561Y63.91\n",
    "N1106 G02X113.278Y64.193I-0.142J0.141\n",
    "N1107 G01X113.561Y64.476\n",
    "N1108 LASER_OFF(0)\n",
    "N1109 SUB1_10:SUB=\"1_10\"; (DXFPUNC0) pz1 Cont=10\n",
    "N1110 G00Z40\n",
    "N1111 G01X77.673Y32.6F130000\n",
    "N1112 LASER_STARTP(1)\n",
    "N1113 LASER_ON(6)\n",
    "N1114 G01X76.778Y34.15\n",
    "N1115 LASER_ON(1)\n",
    "N1116 G02X77.5Y35.4I0.722J0.417\n",
    "N1117 Y29.6I0J-2.9\n",
    "N1118 Y35.4I0J2.9\n",
    "N1119 LASER_OFF(0)\n",
    "N1120 SUB1_11:SUB=\"1_11\"; (DXFPUNC0) pz1 Cont=11\n",
    "N1121 G00Z40\n",
    "N1122 G01X148.173Y106.6F130000\n",
    "N1123 LASER_STARTP(1)\n",
    "N1124 LASER_ON(6)\n",
    "N1125 G01X147.668Y107.475\n",
    "N1126 LASER_ON(1)\n",
    "N1127 G02X148Y108.05I0.332J0.192\n",
    "N1128 Y104.95I0J-1.55\n",
    "N1129 Y108.05I0J1.55\n",
    "N1130 LASER_OFF(0)\n",
    "N1131 SUB1_12:SUB=\"1_12\"; (DXFPUNC0) pz1 Cont=12\n",
    "N1132 G00Z40\n",
    "N1133 G01X148.758Y147.575F130000\n",
    "N1134 LASER_STARTP(1)\n",
    "N1135 LASER_ON(6)\n",
    "N1136 G01Y149.45\n",
    "N1137 LASER_ON(1)\n",
    "N1138 G01Y143.15\n",
    "N1139 G03X149.158I0.2J0\n",
    "N1140 G01Y149.85\n",
    "N1141 G03X148.758I-0.2J0\n",
    "N1142 G01Y149.45\n",
    "N1143 LASER_OFF(0)\n",
    "N1144 SUB1_13:SUB=\"1_13\"; (DXFPUNC0) pz1 Cont=13\n",
    "N1145 G00Z40\n",
    "N1146 G01X148.173Y186.6F130000\n",
    "N1147 LASER_STARTP(1)\n",
    "N1148 LASER_ON(6)\n",
    "N1149 G01X147.668Y187.475\n",
    "N1150 LASER_ON(1)\n",
    "N1151 G02X148Y188.05I0.332J0.192\n",
    "N1152 Y184.95I0J-1.55\n",
    "N1153 Y188.05I0J1.55\n",
    "N1154 LASER_OFF(0)\n",
    "N1155 SUB1_14:SUB=\"1_14\"; (DXFPUNC0) pz1 Cont=14\n",
    "N1156 G00Z40\n",
    "N1157 G01X114.604Y226.916F130000\n",
    "N1158 LASER_STARTP(1)\n",
    "N1159 LASER_ON(6)\n",
    "N1160 G01X113.278Y228.242\n",
    "N1161 LASER_ON(1)\n",
    "N1162 G02X113.561Y228.524I0.141J0.141\n",
    "N1163 G01X118.016Y224.07\n",
    "N1164 G03X118.298Y224.352I0.141J0.141\n",
    "N1165 G01X113.561Y229.09\n",
    "N1166 G03X113.278Y228.807I-0.142J-0.141\n",
    "N1167 G01X113.561Y228.524\n",
    "N1168 LASER_OFF(0)\n",
    "N1169 SUB1_15:SUB=\"1_15\"; (DXFPUNC0) pz1 Cont=15\n",
    "N1170 G00Z40\n",
    "N1171 G01X78.107Y198.31F130000\n",
    "N1172 LASER_STARTP(1)\n",
    "N1173 LASER_ON(6)\n",
    "N1174 G01Y196.143\n",
    "N1175 LASER_ON(1)\n",
    "N1176 G03X79.3Y194.95I1.193J0\n",
    "N1177 G01X84.1\n",
    "N1178 Y201.05\n",
    "N1179 X74.5\n",
    "N1180 Y194.95\n",
    "N1181 X79.3\n",
    "N1182 LASER_OFF(0)\n",
    "N1183 SUB1_16:SUB=\"1_16\"; (DXFPUNC0) pz1 Cont=16\n",
    "N1184 G00Z40\n",
    "N1185 G01X77.673Y260.6F130000\n",
    "N1186 LASER_STARTP(1)\n",
    "N1187 LASER_ON(6)\n",
    "N1188 G01X76.778Y262.15\n",
    "N1189 LASER_ON(1)\n",
    "N1190 G02X77.5Y263.4I0.722J0.417\n",
    "N1191 Y257.6I0J-2.9\n",
    "N1192 Y263.4I0J2.9\n",
    "N1193 LASER_OFF(0)\n",
    "N1194 SUB1_17:SUB=\"1_17\"; (DXFPUNC0) pz1 Cont=17\n",
    "N1195 G00Z40\n",
    "N1196 G01X50.173Y267.8F130000\n",
    "N1197 LASER_STARTP(1)\n",
    "N1198 LASER_ON(6)\n",
    "N1199 G01X49.278Y269.35\n",
    "N1200 LASER_ON(1)\n",
    "N1201 G02X50Y270.6I0.722J0.417\n",
    "N1202 Y264.8I0J-2.9\n",
    "N1203 Y270.6I0J2.9\n",
    "N1204 LASER_OFF(0)\n",
    "N1205 SUB1_18:SUB=\"1_18\"; (DXFPUNC0) pz1 Cont=18\n",
    "N1206 G00Z40\n",
    "N1207 G01X10.173Y214.7F130000\n",
    "N1208 LASER_STARTP(1)\n",
    "N1209 LASER_ON(6)\n",
    "N1210 G01X9.885Y215.2\n",
    "N1211 LASER_ON(1)\n",
    "N1212 G02X10Y215.4I0.115J0.067\n",
    "N1213 Y213.8I0J-0.8\n",
    "N1214 Y215.4I0J0.8\n",
    "N1215 LASER_OFF(0)\n",
    "N1216 SUB1_19:SUB=\"1_19\"; (DXFPUNC0) pz1 Cont=19\n",
    "N1217 G00Z40\n",
    "N1218 G01X10.173Y211.3F130000\n",
    "N1219 LASER_STARTP(1)\n",
    "N1220 LASER_ON(6)\n",
    "N1221 G01X9.885Y211.8\n",
    "N1222 LASER_ON(1)\n",
    "N1223 G02X10Y212I0.115J0.067\n",
    "N1224 Y210.4I0J-0.8\n",
    "N1225 Y212I0J0.8\n",
    "N1226 LASER_OFF(0)\n",
    "N1227 SUB1_20:SUB=\"1_20\"; (DXFPUNC0) pz1 Cont=20\n",
    "N1228 G00Z40\n",
    "N1229 G01X10.173Y207.9F130000\n",
    "N1230 LASER_STARTP(1)\n",
    "N1231 LASER_ON(6)\n",
    "N1232 G01X9.885Y208.4\n",
    "N1233 LASER_ON(1)\n",
    "N1234 G02X10Y208.6I0.115J0.067\n",
    "N1235 Y207I0J-0.8\n",
    "N1236 Y208.6I0J0.8\n",
    "N1237 LASER_OFF(0)\n",
    "N1238 SUB1_21:SUB=\"1_21\"; (DXFPUNC0) pz1 Cont=21\n",
    "N1239 G00Z40\n",
    "N1240 G01X10.173Y204.5F130000\n",
    "N1241 LASER_STARTP(1)\n",
    "N1242 LASER_ON(6)\n",
    "N1243 G01X9.885Y205\n",
    "N1244 LASER_ON(1)\n",
    "N1245 G02X10Y205.2I0.115J0.067\n",
    "N1246 Y203.6I0J-0.8\n",
    "N1247 Y205.2I0J0.8\n",
    "N1248 LASER_OFF(0)\n",
    "N1249 SUB1_22:SUB=\"1_22\"; (DXFPUNC0) pz1 Cont=22\n",
    "N1250 G00Z40\n",
    "N1251 G01X10.5Y-0.2F130000\n",
    "N1252 LASER_STARTP(1)\n",
    "N1253 LASER_ON(6)\n",
    "N1254 G01X15.5\n",
    "N1255 LASER_ON(1)\n",
    "N1256 G01X32.2\n",
    "N1257 Y10.5\n",
    "N1258 X44.7\n",
    "N1259 Y20.1\n",
    "N1260 X56.583\n",
    "N1261 X63.783Y27.3\n",
    "N1262 X93.083\n",
    "N1263 X152.7Y86.917\n",
    "N1264 Y110.8\n",
    "N1265 X162.2\n",
    "N1266 Y182.2\n",
    "N1267 X152.7\n",
    "N1268 Y206.083\n",
    "N1269 X93.083Y265.7\n",
    "N1270 X63.783\n",
    "N1271 X56.583Y272.9\n",
    "N1272 X44.7\n",
    "N1273 Y282.5\n",
    "N1274 X32.2\n",
    "N1275 Y293.2\n",
    "N1276 X15.3\n",
    "N1277 Y283.7\n",
    "N1278 X2.3\n",
    "N1279 Y272.9\n",
    "N1280 X-0.2\n",
    "N1281 Y222.3\n",
    "N1282 X9.8\n",
    "N1283 Y216.2\n",
    "N1284 X-0.2\n",
    "N1285 Y202.8\n",
    "N1286 X9.8\n",
    "N1287 Y196.7\n",
    "N1288 X-0.2\n",
    "N1289 Y95.8\n",
    "N1290 X11.8\n",
    "N1291 Y36.2\n",
    "N1292 X-0.2\n",
    "N1293 Y20.1\n",
    "N1294 X2.3\n",
    "N1295 Y9.3\n",
    "N1296 X15.3\n",
    "N1297 X15.3Y0\n",
    "N1298 LASER_OFF(0)\n",
    "N1299 END:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare two documents with codebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import torch and transformers libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load CodeBERT model and tokenizer\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Define a function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(x, y):\n",
    "    # Compute the norm of each vector\n",
    "    x_norm = torch.norm(x, dim=1, keepdim=True)\n",
    "    y_norm = torch.norm(y, dim=1, keepdim=True)\n",
    "    # Compute the dot product between the vectors and divide by the norms\n",
    "    similarity = torch.matmul(x, y.T) / (x_norm * y_norm)\n",
    "    # Return the similarity matrix\n",
    "    return similarity\n",
    "\n",
    "# Define a function to compare two documents using CodeBERT\n",
    "def compare_documents(doc1, doc2):\n",
    "    # Tokenize and encode the documents\n",
    "    # Tokenize and encode the documents with a maximum length of 512 tokens\n",
    "    input_ids1 = tokenizer(doc1, return_tensors=\"pt\", max_length=512).input_ids\n",
    "    input_ids2 = tokenizer(doc2, return_tensors=\"pt\", max_length=512).input_ids\n",
    "\n",
    "    # Extract the last hidden state of the [CLS] token\n",
    "    output1 = model(input_ids1).last_hidden_state[:, 0, :]\n",
    "    output2 = model(input_ids2).last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Compute the cosine similarity between the outputs\n",
    "    similarity = cosine_similarity(output1, output2)\n",
    "\n",
    "    # Return the similarity score\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codebert finetune\n",
    "https://github.com/microsoft/AzureML-BERT/tree/master/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare doc1 and doc2 and print the similarity score\n",
    "\n",
    "similarity = compare_documents(doc1, doc1)\n",
    "print(f\"The similarity between doc1 and doc2 is {similarity}\")\n",
    "\n",
    "similarity = compare_documents(doc1, doc2)\n",
    "print(f\"The similarity between doc1 and doc2 is {similarity}\")\n",
    "\n",
    "# Compare doc1 and doc3 and print the similarity score\n",
    "similarity = compare_documents(doc1, doc3)\n",
    "print(f\"The similarity between doc1 and doc3 is {similarity}\")\n",
    "\n",
    "# Compare doc1 and doc4 and print the similarity score\n",
    "similarity = compare_documents(doc1, doc4)\n",
    "print(f\"The similarity between doc1 and doc4 is {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForCausalLM, pipeline\n",
    "\n",
    "# Load the RobertaForCausalLM model and tokenizer\n",
    "model = RobertaForCausalLM.from_pretrained(\"microsoft/codebert-base\")\n",
    "# Add max_length and truncation arguments to the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\", max_length=512, truncation=True)\n",
    "\n",
    "# Load a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Define a function to generate an explanation of the difference between two documents using CodeBERT\n",
    "def explain_difference(doc1, doc2):\n",
    "  # Summarize the documents\n",
    "  summary1 = summarizer(doc1, min_length=10, max_length=50)[0][\"summary_text\"]\n",
    "  summary2 = summarizer(doc2, min_length=10, max_length=50)[0][\"summary_text\"]\n",
    "  # Tokenize and encode the summaries\n",
    "  input_ids1 = tokenizer(summary1, return_tensors=\"pt\").input_ids\n",
    "  input_ids2 = tokenizer(summary2, return_tensors=\"pt\").input_ids\n",
    "  # Convert the separator token id to a tensor\n",
    "  sep_tensor = torch.tensor([tokenizer.sep_token_id])\n",
    "  # Add a dimension to the separator tensor\n",
    "  sep_tensor = torch.unsqueeze(sep_tensor, 0)\n",
    "  # Concatenate the summaries with the separator tensor in between\n",
    "  input_ids = torch.cat([input_ids1, sep_tensor, input_ids2], dim=1)\n",
    "  # Generate an explanation of the difference using CodeBERT\n",
    "  # Use max_new_tokens instead of max_length to control the generation length\n",
    "  # Increase max_new_tokens to avoid truncating the input\n",
    "  explanation_ids = model.generate(input_ids, decoder_start_token_id=tokenizer.cls_token_id, max_new_tokens=40)\n",
    "  # Decode the explanation and return it\n",
    "  explanation = tokenizer.decode(explanation_ids[0], skip_special_tokens=True)\n",
    "  return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an explanation of the difference\n",
    "explanation = explain_difference(doc1, doc2)\n",
    "# Print the explanation\n",
    "print(explanation) # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# using longformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and transformers libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load CodeBERT model and tokenizer\n",
    "model = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "# Define a function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(x, y):\n",
    "    # Compute the norm of each vector\n",
    "    x_norm = torch.norm(x, dim=1, keepdim=True)\n",
    "    y_norm = torch.norm(y, dim=1, keepdim=True)\n",
    "    # Compute the dot product between the vectors and divide by the norms\n",
    "    similarity = torch.matmul(x, y.T) / (x_norm * y_norm)\n",
    "    # Return the similarity matrix\n",
    "    return similarity\n",
    "\n",
    "# Define a function to compare two documents using CodeBERT\n",
    "def compare_documents(doc1, doc2):\n",
    "    # Tokenize and encode the documents\n",
    "    # Tokenize and encode the documents with a maximum length of 512 tokens\n",
    "    input_ids1 = tokenizer(doc1, return_tensors=\"pt\", max_length=4000).input_ids\n",
    "    input_ids2 = tokenizer(doc2, return_tensors=\"pt\", max_length=4000).input_ids\n",
    "\n",
    "    # Extract the last hidden state of the [CLS] token\n",
    "    output1 = model(input_ids1).last_hidden_state[:, 0, :]\n",
    "    output2 = model(input_ids2).last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Compute the cosine similarity between the outputs\n",
    "    similarity = cosine_similarity(output1, output2)\n",
    "\n",
    "    # Return the similarity score\n",
    "    return similarity\n",
    "\n",
    "# Compare doc1 and doc2 and print the similarity score\n",
    "similarity = compare_documents(doc1, doc2)\n",
    "print(f\"The similarity between doc1 and doc2 is {similarity}\")\n",
    "\n",
    "# Compare doc1 and doc3 and print the similarity score\n",
    "similarity = compare_documents(doc1, doc3)\n",
    "print(f\"The similarity between doc1 and doc3 is {similarity}\")\n",
    "\n",
    "# Compare doc1 and doc4 and print the similarity score\n",
    "similarity = compare_documents(doc1, doc4)\n",
    "print(f\"The similarity between doc1 and doc4 is {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import requests\n",
    "import json\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# Define a function to generate an explanation of the difference between two documents using the inference endpoint and Bloom\n",
    "def explain_difference(doc1, doc2):\n",
    "  # Summarize the documents\n",
    "  summary1 = summarizer(doc1, min_length=10, max_length=15)[0][\"summary_text\"]\n",
    "  summary2 = summarizer(doc2, min_length=10, max_length=15)[0][\"summary_text\"]\n",
    "  # Use the inference endpoint to compare the documents and generate a summary of their similarity or difference\n",
    "  # Replace with your own endpoint URL and token\n",
    "  endpoint_url = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
    "  headers = {\"Authorization\": \"Bearer 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\", \"Content-Type\": \"application/json\"}\n",
    "  data = json.dumps({\"inputs\": f\"Compare {doc1} and {doc2}\"})\n",
    "  response = requests.post(endpoint_url, headers=headers, data=data)\n",
    "  print(response)\n",
    "  comparison = response.json()[0][\"generated_text\"]\n",
    "  return summary1, summary2, comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate summaries of the documents and a comparison of them\n",
    "summary1, summary2, comparison = explain_difference(doc1, doc2)\n",
    "\n",
    "# Print the summaries and comparison\n",
    "# print(summary1) # Add x and y.\n",
    "# print(summary2) # Multiply x and y.\n",
    "print(comparison) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def generate(text):\n",
    "    result = generator(text, max_length=30, num_return_sequences=1)\n",
    "    return result[0][\"generated_text\"]\n",
    "\n",
    "examples = [\n",
    "    [\"The Moon's orbit around Earth has\"],\n",
    "    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n",
    "]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T_smartquoting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
